{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471c1cd2-4521-40bf-b4a6-aaddf338264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python DANmodels.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c04232c-e36b-4acd-bb21-1b320d9499ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch and related PyTorch libraries\n",
    "import torch\n",
    "from torch import nn  # Neural networks\n",
    "import torch.nn.functional as F  # Functional API\n",
    "from torch.utils.data import Dataset, DataLoader  # Data utilities\n",
    "\n",
    "# NLP Preprocessing Libraries (nltk)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Tokenizers\n",
    "from nltk.corpus import stopwords  # Stop words\n",
    "from nltk.stem import PorterStemmer  # Stemmer\n",
    "\n",
    "\n",
    "# Utility and Helper Libraries\n",
    "from typing import List  # Type hinting\n",
    "import collections\n",
    "from collections import Counter, defaultdict  # Counting elements\n",
    "import re  # Regular expressions\n",
    "import numpy as np  # Numerical computations\n",
    "import time  # Time tracking\n",
    "import argparse  # Argument parsing\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476a4a25-a319-4404-becb-577e447b13de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentExample:\n",
    "    \"\"\"\n",
    "    Data wrapper for a single example for sentiment analysis.\n",
    "\n",
    "    Attributes:\n",
    "        words (List[string]): list of words\n",
    "        label (int): 0 or 1 (0 = negative, 1 = positive)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, words, label):\n",
    "        self.words = words\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.words) + \"; label=\" + repr(self.label)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fccd899-ae69-4008-bd3f-9648d938ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentiment_examples(infile: str) -> List[SentimentExample]:\n",
    "    \"\"\"\n",
    "    Reads sentiment examples in the format [0 or 1]<TAB>[raw sentence]; tokenizes and cleans the sentences and forms\n",
    "    SentimentExamples.\n",
    "\n",
    "    Note that we lowercase the data for you. This is because the GloVe embeddings don't\n",
    "    distinguish case and so can only be used with lowercasing.\n",
    "\n",
    "    :param infile: file to read from\n",
    "    :return: a list of SentimentExamples parsed from the file\n",
    "    \"\"\"\n",
    "    f = open(infile)\n",
    "    exs = []\n",
    "    for line in f:\n",
    "        if len(line.strip()) > 0:\n",
    "            fields = line.split(\"\\t\")\n",
    "            if len(fields) != 2:\n",
    "                fields = line.split()\n",
    "                label = 0 if \"0\" in fields[0] else 1\n",
    "                sent = \" \".join(fields[1:]).lower()\n",
    "            else:\n",
    "                # Slightly more robust to reading bad output than int(fields[0])\n",
    "                label = 0 if \"0\" in fields[0] else 1\n",
    "                sent = fields[1].lower()\n",
    "            tokenized_cleaned_sent = list(filter(lambda x: x != '', sent.rstrip().split(\" \")))\n",
    "            exs.append(SentimentExample(tokenized_cleaned_sent, label))\n",
    "    f.close()\n",
    "    return exs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8002b8ff-e057-4b76-9fa4-3029848c50b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/anaykulkarni/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ca5c6372-39bf-4ad0-8345-adaca441c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get symbol pairs and their frequencies\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return pairs\n",
    "\n",
    "# Function to merge the most frequent pair\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "# Number of merge operations\n",
    "# num_merges = 10\n",
    "\n",
    "def initialize_vocab(infile):\n",
    "    vocab = defaultdict(int)\n",
    "    exs = read_sentiment_examples(infile)\n",
    "    for ex in exs:\n",
    "        for word in ex.words:\n",
    "            subword_format = ' '.join(list(word)) + ' </w>'\n",
    "            vocab[subword_format] += 1\n",
    "    return vocab\n",
    "\n",
    "def perform_bpe_merges(num_merges, vocab): \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        #print(f'Merge {i + 1}: {best}')\n",
    "    return vocab\n",
    "\n",
    "def get_subword_vocab(bpe_vocab):\n",
    "    subword_vocab = defaultdict(int)\n",
    "    for word, count in bpe_vocab.items():\n",
    "        # Remove the '</w>' marker\n",
    "        subwords = word.replace('</w>', '').split()\n",
    "        # Count each subword\n",
    "        for subword in subwords:\n",
    "            subword_vocab[subword] += count\n",
    "    return subword_vocab\n",
    "\n",
    "def build_bpe_subword_vocab(infile, merges):\n",
    "    word_vocab = initialize_vocab(infile)\n",
    "    pre_bpe_subword_vocab = get_subword_vocab(word_vocab)\n",
    "\n",
    "    print('Before running BPE algorithm-')\n",
    "    print('Word-level vocab size: ', len(word_vocab))\n",
    "    print('Subword-level vocab size: ', len(pre_bpe_subword_vocab))\n",
    "\n",
    "    word_vocab = perform_bpe_merges(merges, word_vocab)\n",
    "    post_bpe_subword_vocab = get_subword_vocab(word_vocab)\n",
    "\n",
    "    print('After running BPE algorithm-')\n",
    "    print('Word-level vocab size: ', len(word_vocab))\n",
    "    print('Subword-level vocab size: ', len(post_bpe_subword_vocab))\n",
    "    print('Compression Ratio: ', len(word_vocab)/len(post_bpe_subword_vocab))\n",
    "\n",
    "    return post_bpe_subword_vocab\n",
    "\n",
    "def tokenize_bpe(text, vocab):\n",
    "    # Tokenize text using BPE vocabulary\n",
    "    words = text.split()\n",
    "    tokenized_text = []\n",
    "\n",
    "    # For each word in the list of words\n",
    "    for word in words:\n",
    "        subword = []\n",
    "        i = 0\n",
    "        # i index starts from beginning of word \n",
    "        while i < len(word):\n",
    "            # Look for the longest subword in the vocabulary that matches the word prefix\n",
    "            found = False\n",
    "            #  and j index from the back.\n",
    "            for j in range(len(word), i, -1):\n",
    "                sub_token = word[i:j]\n",
    "                if sub_token in vocab:\n",
    "                    subword.append(sub_token)\n",
    "                    i = j  # Move index past the subword\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                subword.append(word[i])  # Add the character if no subword is found\n",
    "                i += 1\n",
    "                \n",
    "        tokenized_text.append(' '.join(subword)) # Join as a sentence of subwords\n",
    "    \n",
    "    return ' '.join(tokenized_text) # return tokenized text as a sentence(s) of subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7b1ccb4c-b616-4cae-a4bc-2aa3226e49f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before running BPE algorithm-\n",
      "Word-level vocab size:  14830\n",
      "Subword-level vocab size:  64\n",
      "After running BPE algorithm-\n",
      "Word-level vocab size:  14830\n",
      "Subword-level vocab size:  471\n",
      "Compression Ratio:  31.48619957537155\n",
      "the 6209\n",
      "r 1367\n",
      "oc 525\n",
      "k 2398\n",
      "is 3140\n",
      "de 1008\n",
      "st 1995\n",
      "in 4213\n",
      "ed 2496\n",
      "to 2687\n",
      "be 1281\n",
      "2 118\n",
      "1 177\n",
      "c 3291\n",
      "ent 1146\n",
      "ur 500\n",
      "y 3875\n",
      "'s 2025\n",
      "new 133\n",
      "`` 210\n",
      "con 660\n",
      "an 2994\n",
      "'' 209\n",
      "and 4288\n",
      "that 1613\n",
      "he 324\n",
      "go 161\n",
      "ing 2968\n",
      "make 176\n",
      "a 7236\n",
      "s 5961\n",
      "pl 953\n",
      "as 2145\n",
      "h 2652\n",
      "even 260\n",
      "gre 196\n",
      "at 1598\n",
      "er 3580\n",
      "than 400\n",
      "ar 1936\n",
      "n 2214\n",
      "old 168\n",
      "ch 1699\n",
      "w 1740\n",
      "z 460\n",
      "en 2690\n",
      "e 4240\n",
      "g 2410\n",
      ", 5885\n",
      "j 605\n",
      "- 2035\n",
      "cl 363\n",
      "au 496\n",
      "d 4220\n",
      "v 1578\n",
      "am 802\n",
      "me 317\n",
      "or 2050\n",
      "se 1061\n",
      "al 2440\n",
      ". 6732\n",
      "ge 496\n",
      "ous 491\n",
      "ly 1255\n",
      "el 1447\n",
      "ab 333\n",
      "ate 293\n",
      "t 4144\n",
      "u 1761\n",
      "ation 323\n",
      "of 3952\n",
      "l 2906\n",
      "ings 165\n",
      "tr 449\n",
      "il 828\n",
      "og 164\n",
      "so 420\n",
      "ol 692\n",
      "um 366\n",
      "wor 415\n",
      "ds 376\n",
      "can 204\n",
      "not 439\n",
      "qu 596\n",
      "ately 141\n",
      "des 193\n",
      "ri 839\n",
      "o 2396\n",
      "wr 227\n",
      "it 2984\n",
      "\\ 87\n",
      "/ 70\n",
      "director 191\n",
      "pe 264\n",
      "ter 412\n",
      "ack 318\n",
      "on 2176\n",
      "ex 902\n",
      "p 3138\n",
      "vis 145\n",
      "i 2910\n",
      "m 3777\n",
      "id 364\n",
      "le 1249\n",
      "ear 273\n",
      "th 1778\n",
      "com 831\n",
      "pos 157\n",
      "br 312\n",
      "ad 867\n",
      "tri 160\n",
      "bu 231\n",
      "es 2375\n",
      "-- 424\n",
      "fe 281\n",
      "po 342\n",
      "ten 157\n",
      "ti 984\n",
      "its 846\n",
      "more 424\n",
      "si 793\n",
      "int 324\n",
      "ve 418\n",
      "story 339\n",
      "but 885\n",
      "wh 509\n",
      "age 207\n",
      "ertain 36\n",
      "ap 637\n",
      "ure 574\n",
      "end 333\n",
      "sp 516\n",
      "ir 667\n",
      "pi 206\n",
      "ec 717\n",
      "et 436\n",
      "ac 654\n",
      "sti 148\n",
      "ll 169\n",
      "char 155\n",
      "ere 288\n",
      "ther 247\n",
      "you 766\n",
      "' 670\n",
      "re 2249\n",
      "li 936\n",
      "gh 290\n",
      "by 483\n",
      "any 174\n",
      "der 273\n",
      "other 200\n",
      "sel 261\n",
      "f 2699\n",
      "un 1207\n",
      "ably 140\n",
      "cin 194\n",
      "ating 214\n",
      "ay 427\n",
      "ful 457\n",
      "ow 1018\n",
      "just 244\n",
      "our 299\n",
      "cre 214\n",
      "ness 157\n",
      "ima 158\n",
      "this 830\n",
      "os 342\n",
      "ma 796\n",
      "ght 183\n",
      "ast 424\n",
      "par 299\n",
      "ou 436\n",
      "ob 328\n",
      "vi 420\n",
      "with 953\n",
      "hum 230\n",
      "scre 198\n",
      "str 332\n",
      "uc 250\n",
      "ted 180\n",
      "em 954\n",
      "op 573\n",
      "ta 380\n",
      "tions 209\n",
      "good 225\n",
      "fun 146\n",
      "action 137\n",
      "act 329\n",
      "di 878\n",
      "gu 332\n",
      "ace 167\n",
      "ra 764\n",
      "ph 213\n",
      "dra 121\n",
      "like 439\n",
      "fl 352\n",
      "ic 1382\n",
      "ost 52\n",
      "some 447\n",
      "ces 234\n",
      "jo 224\n",
      "aw 135\n",
      "ish 133\n",
      "tt 401\n",
      "gen 191\n",
      "for 1247\n",
      "dis 247\n",
      "ov 205\n",
      "out 423\n",
      "x 200\n",
      "prov 144\n",
      "ine 249\n",
      "b 2138\n",
      "charact 112\n",
      "stu 136\n",
      "é 61\n",
      "tur 149\n",
      "if 304\n",
      "love 145\n",
      "all 767\n",
      "pr 680\n",
      "ty 152\n",
      "has 436\n",
      "been 152\n",
      "itt 159\n",
      "wel 218\n",
      "od 400\n",
      "! 62\n",
      "gr 293\n",
      "fir 144\n",
      "del 168\n",
      "very 163\n",
      "e- 153\n",
      "are 569\n",
      "tic 704\n",
      "they 165\n",
      "what 280\n",
      "makes 150\n",
      "-lrb- 286\n",
      "-rrb- 287\n",
      "king 365\n",
      "thing 403\n",
      "movie 827\n",
      "ul 679\n",
      "ting 146\n",
      "into 249\n",
      "own 259\n",
      "work 164\n",
      "n't 535\n",
      "one 734\n",
      "never 158\n",
      "much 209\n",
      "sion 170\n",
      "ke 321\n",
      "est 400\n",
      "ice 131\n",
      "little 174\n",
      "film 1196\n",
      "kes 114\n",
      "us 495\n",
      "life 153\n",
      "through 129\n",
      "hear 170\n",
      "ts 596\n",
      "min 288\n",
      "ks 366\n",
      "up 267\n",
      "time 227\n",
      "pre 362\n",
      "ays 154\n",
      "most 329\n",
      "wil 100\n",
      "dr 156\n",
      "im 419\n",
      "over 180\n",
      "we 478\n",
      "ess 488\n",
      "ative 135\n",
      "inter 173\n",
      "ence 335\n",
      "... 525\n",
      "ut 248\n",
      "mar 276\n",
      "their 181\n",
      "ated 193\n",
      "pres 169\n",
      "ot 264\n",
      "who 256\n",
      "ever 429\n",
      "oun 303\n",
      "there 260\n",
      "á 2\n",
      "bi 190\n",
      "from 388\n",
      "hi 208\n",
      "performan 194\n",
      "su 692\n",
      "ers 580\n",
      "bo 276\n",
      "ore 89\n",
      "; 75\n",
      "less 210\n",
      "9 121\n",
      "0 192\n",
      "was 135\n",
      "? 100\n",
      "feel 133\n",
      "3 43\n",
      "sh 1006\n",
      "ould 236\n",
      "ser 197\n",
      "ious 164\n",
      "ff 320\n",
      "only 171\n",
      "made 129\n",
      "tion 747\n",
      "ally 569\n",
      "fam 145\n",
      "eas 193\n",
      "om 605\n",
      "ag 486\n",
      "bl 295\n",
      "ev 437\n",
      "í 1\n",
      "tal 181\n",
      "ó 2\n",
      "ough 276\n",
      "ict 169\n",
      "able 496\n",
      "ant 301\n",
      "comedy 219\n",
      "car 148\n",
      "tim 309\n",
      "ity 447\n",
      "ce 289\n",
      "ely 408\n",
      "` 206\n",
      "year 158\n",
      "per 479\n",
      "* 17\n",
      "ical 192\n",
      "would 149\n",
      "wat 197\n",
      "use 148\n",
      "ble 206\n",
      "ill 215\n",
      "tly 137\n",
      "ose 209\n",
      "ves 215\n",
      "ight 184\n",
      "te 210\n",
      "will 214\n",
      "your 163\n",
      "ld 86\n",
      "movies 131\n",
      "too 254\n",
      "ard 215\n",
      "cr 170\n",
      "drama 132\n",
      "thr 175\n",
      "have 370\n",
      "ect 189\n",
      "ep 312\n",
      "ents 214\n",
      "enough 144\n",
      "movi 37\n",
      "res 142\n",
      "ver 195\n",
      "wi 221\n",
      "fr 161\n",
      "sub 177\n",
      "way 239\n",
      "sa 205\n",
      "no 520\n",
      "sc 468\n",
      "fa 196\n",
      "best 132\n",
      "rec 114\n",
      "ying 150\n",
      "about 434\n",
      "characters 199\n",
      "ance 158\n",
      "sur 204\n",
      "ching 152\n",
      "sl 155\n",
      "spec 131\n",
      "igh 204\n",
      "fu 55\n",
      "ood 156\n",
      "tra 247\n",
      "while 139\n",
      "stor 120\n",
      "fin 212\n",
      "ust 52\n",
      "ig 222\n",
      "fre 160\n",
      "mov 90\n",
      "entertain 133\n",
      "tw 169\n",
      "ary 241\n",
      "tive 187\n",
      "ies 158\n",
      "his 355\n",
      "ang 137\n",
      "8 48\n",
      "does 244\n",
      "erta 28\n",
      "oo 412\n",
      "funny 205\n",
      "lif 31\n",
      "which 156\n",
      "direc 66\n",
      "gi 254\n",
      "ong 232\n",
      "ise 217\n",
      "direct 70\n",
      "ial 230\n",
      "off 150\n",
      "man 397\n",
      "som 21\n",
      "ite 200\n",
      "oll 189\n",
      "mat 164\n",
      "ha 172\n",
      "bec 164\n",
      "ling 317\n",
      "loo 187\n",
      "ment 133\n",
      "ingly 218\n",
      "ile 49\n",
      "sen 214\n",
      "fully 152\n",
      ": 115\n",
      "ily 138\n",
      "may 139\n",
      "get 134\n",
      "7 39\n",
      "rom 138\n",
      "pa 205\n",
      "do 253\n",
      "iz 139\n",
      "bad 153\n",
      "5 53\n",
      "edy 28\n",
      "4 34\n",
      "por 179\n",
      "rip 149\n",
      "fil 57\n",
      "ne 292\n",
      "6 28\n",
      "litt 8\n",
      "ü 2\n",
      "lov 53\n",
      "mu 64\n",
      "-r 31\n",
      "$ 8\n",
      "ny 32\n",
      "ich 11\n",
      "perfor 19\n",
      "& 14\n",
      "-l 46\n",
      "â 4\n",
      "ñ 3\n",
      "ï 3\n",
      "# 4\n",
      "æ 1\n",
      "+ 1\n",
      "q 11\n",
      "è 2\n",
      "= 1\n",
      "ã 1\n"
     ]
    }
   ],
   "source": [
    "vocab = build_bpe_subword_vocab('data/train.txt', 500)\n",
    "for word, freq in vocab.items():\n",
    "    print(word, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2995d70-b058-49dd-b6bd-da7ec8b9f128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
