{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04f2820-5db7-4045-b41c-ab31947785c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/anaykulkarni/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "Deep Averaging Network (DAN) with subword tokenization using Byte Pair Encoding:\n",
      "----Loading Training Data----\n",
      "Before running BPE algorithm-------\n",
      "\tWord-level vocab size:  14830\n",
      "\tSubword-level vocab size:  64\n",
      "After running BPE algorithm--------\n",
      "\tWord-level vocab size:  14830\n",
      "\tSubword-level vocab size:  1824\n",
      "\n",
      "Compression Ratio:  8.130482456140351\n",
      "----Loading Test Data--------\n",
      "Before running BPE algorithm-------\n",
      "\tWord-level vocab size:  4339\n",
      "\tSubword-level vocab size:  53\n",
      "After running BPE algorithm--------\n",
      "\tWord-level vocab size:  4339\n",
      "\tSubword-level vocab size:  1747\n",
      "\n",
      "Compression Ratio:  2.483686319404694\n",
      "\n",
      "Data loaded in : 43.679264068603516 seconds\n",
      "\n",
      "Training (from scratch) and Evaluating a 3 Layer Neural Network using BPE Subword-Level tokenization on random weight embeddings\n",
      "Epoch #5: train accuracy 0.578, test accuracy 0.499\n",
      "Epoch #10: train accuracy 0.701, test accuracy 0.495\n",
      "Epoch #15: train accuracy 0.753, test accuracy 0.501\n",
      "Epoch #20: train accuracy 0.796, test accuracy 0.502\n",
      "Epoch #25: train accuracy 0.810, test accuracy 0.515\n",
      "Epoch #30: train accuracy 0.827, test accuracy 0.509\n",
      "Epoch #35: train accuracy 0.836, test accuracy 0.505\n",
      "Epoch #40: train accuracy 0.851, test accuracy 0.514\n",
      "Epoch #45: train accuracy 0.857, test accuracy 0.516\n",
      "Epoch #50: train accuracy 0.862, test accuracy 0.519\n",
      "Epoch #55: train accuracy 0.861, test accuracy 0.523\n",
      "Epoch #60: train accuracy 0.879, test accuracy 0.513\n",
      "Epoch #65: train accuracy 0.871, test accuracy 0.501\n",
      "Epoch #70: train accuracy 0.880, test accuracy 0.518\n",
      "Epoch #75: train accuracy 0.879, test accuracy 0.510\n",
      "Epoch #80: train accuracy 0.883, test accuracy 0.513\n",
      "Epoch #85: train accuracy 0.877, test accuracy 0.505\n",
      "Epoch #90: train accuracy 0.890, test accuracy 0.514\n",
      "Epoch #95: train accuracy 0.895, test accuracy 0.514\n",
      "Epoch #100: train accuracy 0.892, test accuracy 0.506\n",
      "\n",
      "Training completed in : 70.63979816436768 seconds\n",
      "Evaluation completed in : 1.7287065982818604 seconds\n",
      "\n",
      "----Loading Training Data----\n",
      "Read in 14923 vectors of size 300\n",
      "----Loading Test Data--------\n",
      "Read in 14923 vectors of size 300\n",
      "Data loaded in : 1.147217035293579 seconds\n",
      "\n",
      "Training (from scratch) and Evaluating a 3 Layer Neural Network using Word-Level tokenization on random weight embeddings\n",
      "Epoch #5: train accuracy 0.606, test accuracy 0.624\n",
      "Epoch #10: train accuracy 0.733, test accuracy 0.711\n",
      "Epoch #15: train accuracy 0.806, test accuracy 0.714\n",
      "Epoch #20: train accuracy 0.857, test accuracy 0.753\n",
      "Epoch #25: train accuracy 0.885, test accuracy 0.740\n",
      "Epoch #30: train accuracy 0.905, test accuracy 0.744\n",
      "Epoch #35: train accuracy 0.918, test accuracy 0.753\n",
      "Epoch #40: train accuracy 0.920, test accuracy 0.753\n",
      "Epoch #45: train accuracy 0.936, test accuracy 0.765\n",
      "Epoch #50: train accuracy 0.938, test accuracy 0.722\n",
      "Epoch #55: train accuracy 0.944, test accuracy 0.751\n",
      "Epoch #60: train accuracy 0.950, test accuracy 0.759\n",
      "Epoch #65: train accuracy 0.954, test accuracy 0.756\n",
      "Epoch #70: train accuracy 0.957, test accuracy 0.764\n",
      "Epoch #75: train accuracy 0.953, test accuracy 0.761\n",
      "Epoch #80: train accuracy 0.957, test accuracy 0.753\n",
      "Epoch #85: train accuracy 0.961, test accuracy 0.772\n",
      "Epoch #90: train accuracy 0.962, test accuracy 0.769\n",
      "Epoch #95: train accuracy 0.956, test accuracy 0.769\n",
      "Epoch #100: train accuracy 0.965, test accuracy 0.767\n",
      "\n",
      "Training completed in : 352.515172958374 seconds\n",
      "Evaluation completed in : 1.9261095523834229 seconds\n",
      "\n",
      "\n",
      "Plot saved as Figure6-train.png\n",
      "\n",
      "Plot saved as Figure6-test.png\n"
     ]
    }
   ],
   "source": [
    "!python DANmodels.py --model 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b229d6c4-171e-4f52-b1e8-72585432674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch and related PyTorch libraries\n",
    "import torch\n",
    "from torch import nn  # Neural networks\n",
    "import torch.nn.functional as F  # Functional API\n",
    "from torch.utils.data import Dataset, DataLoader  # Data utilities\n",
    "\n",
    "# NLP Preprocessing Libraries (nltk)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Tokenizers\n",
    "from nltk.corpus import stopwords  # Stop words\n",
    "from nltk.stem import PorterStemmer  # Stemmer\n",
    "\n",
    "\n",
    "# Utility and Helper Libraries\n",
    "from typing import List  # Type hinting\n",
    "import collections\n",
    "from collections import Counter, defaultdict  # Counting elements\n",
    "import re  # Regular expressions\n",
    "import numpy as np  # Numerical computations\n",
    "import time  # Time tracking\n",
    "import argparse  # Argument parsing\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a337e058-b35c-453b-ae3e-425aea86268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentExample:\n",
    "    \"\"\"\n",
    "    Data wrapper for a single example for sentiment analysis.\n",
    "\n",
    "    Attributes:\n",
    "        words (List[string]): list of words\n",
    "        label (int): 0 or 1 (0 = negative, 1 = positive)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, words, label):\n",
    "        self.words = words\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.words) + \"; label=\" + repr(self.label)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb6c5e3d-9ebc-459b-978f-6530a86b68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentiment_examples(infile: str) -> List[SentimentExample]:\n",
    "    \"\"\"\n",
    "    Reads sentiment examples in the format [0 or 1]<TAB>[raw sentence]; tokenizes and cleans the sentences and forms\n",
    "    SentimentExamples.\n",
    "\n",
    "    Note that we lowercase the data for you. This is because the GloVe embeddings don't\n",
    "    distinguish case and so can only be used with lowercasing.\n",
    "\n",
    "    :param infile: file to read from\n",
    "    :return: a list of SentimentExamples parsed from the file\n",
    "    \"\"\"\n",
    "    f = open(infile)\n",
    "    exs = []\n",
    "    for line in f:\n",
    "        if len(line.strip()) > 0:\n",
    "            fields = line.split(\"\\t\")\n",
    "            if len(fields) != 2:\n",
    "                fields = line.split()\n",
    "                label = 0 if \"0\" in fields[0] else 1\n",
    "                sent = \" \".join(fields[1:]).lower()\n",
    "            else:\n",
    "                # Slightly more robust to reading bad output than int(fields[0])\n",
    "                label = 0 if \"0\" in fields[0] else 1\n",
    "                sent = fields[1].lower()\n",
    "            tokenized_cleaned_sent = list(filter(lambda x: x != '', sent.rstrip().split(\" \")))\n",
    "            exs.append(SentimentExample(tokenized_cleaned_sent, label))\n",
    "    f.close()\n",
    "    return exs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ece896ae-ebc2-426b-af7f-f35410c2f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get symbol pairs and their frequencies\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return pairs\n",
    "\n",
    "# Function to merge the most frequent pair\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "# Number of merge operations\n",
    "# num_merges = 10\n",
    "\n",
    "def initialize_vocab(infile):\n",
    "    vocab = defaultdict(int)\n",
    "    exs = read_sentiment_examples(infile)\n",
    "    for ex in exs:\n",
    "        for word in ex.words:\n",
    "            subword_format = ' '.join(list(word)) + ' </w>'\n",
    "            vocab[subword_format] += 1\n",
    "    return vocab\n",
    "\n",
    "def perform_bpe_merges(num_merges, vocab): \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        #print(f'Merge {i + 1}: {best}')\n",
    "    return vocab\n",
    "\n",
    "def get_subword_vocab(bpe_vocab):\n",
    "    subword_vocab = defaultdict(int)\n",
    "    for word, count in bpe_vocab.items():\n",
    "        # Remove the '</w>' marker\n",
    "        subwords = word.replace('</w>', '').split()\n",
    "        # Count each subword\n",
    "        for subword in subwords:\n",
    "            subword_vocab[subword] += count\n",
    "    return subword_vocab\n",
    "\n",
    "def build_bpe_subword_vocab(infile, merges):\n",
    "    word_vocab = initialize_vocab(infile)\n",
    "    pre_bpe_subword_vocab = get_subword_vocab(word_vocab)\n",
    "\n",
    "    print('Before running BPE algorithm-')\n",
    "    print('Word-level vocab size: ', len(word_vocab))\n",
    "    print('Subword-level vocab size: ', len(pre_bpe_subword_vocab))\n",
    "\n",
    "    word_vocab = perform_bpe_merges(merges, word_vocab)\n",
    "    post_bpe_subword_vocab = get_subword_vocab(word_vocab)\n",
    "\n",
    "    print('After running BPE algorithm-')\n",
    "    print('Word-level vocab size: ', len(word_vocab))\n",
    "    print('Subword-level vocab size: ', len(post_bpe_subword_vocab))\n",
    "    print('Compression Ratio: ', len(word_vocab)/len(post_bpe_subword_vocab))\n",
    "\n",
    "    return post_bpe_subword_vocab\n",
    "\n",
    "def tokenize_bpe(text, vocab):\n",
    "    # Tokenize text using BPE vocabulary\n",
    "    words = text.split()\n",
    "    tokenized_text = []\n",
    "\n",
    "    # For each word in the list of words\n",
    "    for word in words:\n",
    "        subword = []\n",
    "        i = 0\n",
    "        # i index starts from beginning of word \n",
    "        while i < len(word):\n",
    "            # Look for the longest subword in the vocabulary that matches the word prefix\n",
    "            found = False\n",
    "            #  and j index from the back.\n",
    "            for j in range(len(word), i, -1):\n",
    "                sub_token = word[i:j]\n",
    "                if sub_token in vocab:\n",
    "                    subword.append(sub_token)\n",
    "                    i = j  # Move index past the subword\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                subword.append(word[i])  # Add the character if no subword is found\n",
    "                i += 1\n",
    "                \n",
    "        tokenized_text.append(' '.join(subword)) # Join as a sentence of subwords\n",
    "    \n",
    "    return ' '.join(tokenized_text) # return tokenized text as a sentence(s) of subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3dc3cf46-b8fc-43d3-8089-9e77940eef61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before running BPE algorithm-\n",
      "Word-level vocab size:  14830\n",
      "Subword-level vocab size:  64\n",
      "After running BPE algorithm-\n",
      "Word-level vocab size:  14830\n",
      "Subword-level vocab size:  4447\n",
      "Compression Ratio:  3.3348324713289856\n"
     ]
    }
   ],
   "source": [
    "myvocab = build_bpe_subword_vocab('data/train.txt', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e48cf35-cc3a-4ee9-a6f7-9dbfd6291e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_sentiment_examples('data/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5cc64a4f-f3ab-42d4-aa25-e1aaea1ab827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century 's new `` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "1\n",
      "the gorgeously elaborate continuation of `` the lord of the rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\/director peter jackson 's expanded vision of j.r.r. tolkien 's middle-earth .\n",
      "1\n",
      "singer\\/composer bryan adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .\n",
      "1\n",
      "yet the act is still charming here .\n",
      "1\n",
      "whether or not you 're enlightened by any of derrida 's lectures on `` the other '' and `` the self , '' derrida is an undeniably fascinating and playful fellow .\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "counter = 5\n",
    "for row in dataset:\n",
    "    if counter > 0:\n",
    "        print(\" \".join(row.words))\n",
    "        print(row.label)\n",
    "        counter -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e8424a3-afbe-41be-83d1-c0eaa76d29df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yet the act is still charming here .'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(dataset[3].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff377fcf-8b2d-490a-a0b1-af021793d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = tokenize_bpe(\" \".join(dataset[3].words), myvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69bd4e90-0f54-40fc-ab2c-51e0f243feae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yet', 'the', 'act', 'is', 'still', 'charming', 'here', '.']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9bc28954-9b5b-475e-b92d-e3aed9235641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[126, 0, 127, 2, 128, 129, 130, 50]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[list(myvocab.keys()).index(word) for word in doc.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f655c94-ccf0-49e9-b99b-abbf31d5b8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
